{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T22:58:52.164757Z",
     "start_time": "2019-05-11T22:58:52.154913Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "from scipy.stats import weightedtau, kendalltau\n",
    "from scipy.stats import norm\n",
    "from scipy.linalg import null_space\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse.linalg import eigs\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from copy import deepcopy\n",
    "\n",
    "import trueskill\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First, parse game data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T22:59:18.581419Z",
     "start_time": "2019-05-11T22:58:54.012537Z"
    }
   },
   "outputs": [],
   "source": [
    "dt_str = '%d %B %Y %H:%M:%S'\n",
    "\n",
    "dt_lim = datetime.strptime('06 August 2004 18:13:50', dt_str) #first game in HeadToHead\n",
    "\n",
    "players = set()\n",
    "\n",
    "# each entry is a tuple ([list of players], [list of scores])\n",
    "matches = []\n",
    "\n",
    "# needed for iteration\n",
    "cur_game = -1\n",
    "cur_players = []\n",
    "cur_scores = []\n",
    "\n",
    "# also, filter out all games where every player has no score!\n",
    "with open('/Users/andy/Desktop/7332finalproj/hypergraph-halo-ranking/FreeForAll.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    for row in csv_reader:\n",
    "        date = datetime.strptime(row[0], dt_str)\n",
    "        score = int(row[6])\n",
    "        if date < dt_lim:\n",
    "            game = int(row[1])\n",
    "            player = row[4]\n",
    "            score = int(row[6])\n",
    "            \n",
    "            # next, decide if this row is from the same match\n",
    "            # as the last row, or a different match\n",
    "            if game == cur_game:\n",
    "                cur_players.append(player)\n",
    "                cur_scores.append(score)\n",
    "            else:\n",
    "                if cur_game > 0 and np.sum(np.abs(cur_scores)):\n",
    "                    # append cur_players, cur_scores to matches\n",
    "                    matches.append((cur_players, cur_scores))\n",
    "                    # add cur_players to players\n",
    "                    players.update(cur_players)\n",
    "\n",
    "                # reset cur_game, cur_players, cur_scores\n",
    "                cur_game = game\n",
    "                cur_players = [player]\n",
    "                cur_scores = [score]\n",
    "        else:\n",
    "            break\n",
    "\n",
    "players=list(players) # list of players"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper function for computing PageRank rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T22:59:18.592065Z",
     "start_time": "2019-05-11T22:59:18.585014Z"
    }
   },
   "outputs": [],
   "source": [
    "##################################################\n",
    "# COMPUTE PAGERANK\n",
    "##################################################\n",
    "\n",
    "# given probability transition matrix P\n",
    "# where P_{v,w} = Prob(w -> v)\n",
    "# find pagerank scores with restart probability r\n",
    "def compute_pr(P, r, n, eps=1e-8):\n",
    "    x = np.ones(n) / n*1.0\n",
    "    flag = True\n",
    "    t=0\n",
    "    while flag:\n",
    "        x_new = (1-r)*P*x\n",
    "        x_new = x_new + np.ones(n) * r / n\n",
    "        diff = np.linalg.norm(x_new - x)\n",
    "        if np.linalg.norm(x_new - x,ord=1) < eps and t > 100:\n",
    "            flag = False\n",
    "        t=t+1\n",
    "        x = x_new\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create hypergraph rankings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rankings are a |V| x 1 vector, where the v-th entry is the PageRank score (or TrueSkill rating)  of vertex v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T23:01:52.900105Z",
     "start_time": "2019-05-11T23:00:57.558053Z"
    }
   },
   "outputs": [],
   "source": [
    "pi_list = matches\n",
    "universe = np.array(list(players))\n",
    "# first create these matrices\n",
    "# R = |E| x |V|, R(e, v) = lambda_e(v)\n",
    "# W = |V| x |E|, W(v, e) = w(e) 1(v in e)\n",
    "    \n",
    "m = len(pi_list) # number of hyperedges\n",
    "n = len(universe) # number of items to be ranked \n",
    "R = np.zeros([m, n])\n",
    "W = np.zeros([n, m])\n",
    "\n",
    "for i in range(len(pi_list)):\n",
    "    pi, scores = pi_list[i]\n",
    "    if len(pi) > 1:   \n",
    "        for j in range(len(pi)):\n",
    "            v = pi[j]\n",
    "            v = np.where(universe == v)[0][0] #equivalent to universe.index(v) but for np arrays\n",
    "            R[i, v] = np.exp(scores[j])\n",
    "            W[v,i] = (np.std(scores) + 1.0)\n",
    "        R[i, :] = R[i,:] / sum(R[i,:])\n",
    "\n",
    "# first, normalize W\n",
    "Wnorm=W/W.sum(axis=1)[:,None]\n",
    "Ws = sparse.csr_matrix(Wnorm)\n",
    "Rs = sparse.csr_matrix(R)\n",
    "\n",
    "# create prob trans matrices\n",
    "P = np.transpose(Ws.dot(Rs))\n",
    "\n",
    "# create rankings\n",
    "r=0.40\n",
    "rankings_hg = compute_pr(P, r, n, eps=1e-8).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create G^H, MC3 rankings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G^H rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T23:09:01.371821Z",
     "start_time": "2019-05-11T23:08:36.283463Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create matrix A, where A_{u,v} is given in Eq 10\n",
    "def compute_gh_weights(R, W, P):\n",
    "    E, V = R.shape\n",
    "    A = np.zeros([V,V]) # to return\n",
    "    \n",
    "    # first, create edge weight vector\n",
    "    WE = np.zeros(E)\n",
    "    # for each edge, find first non-zero value that is >0\n",
    "    for e in range(E):\n",
    "        WE[e] = W[np.where(W[:,e] > 0)[0],e][0]\n",
    "    \n",
    "    # iterate over edges, add w(e) * gam_e(u) * gam_e(v) term\n",
    "    # for each pair of vertices u,v \\in e\n",
    "    for e in range(E):\n",
    "        nodes_in_e = np.nonzero(R[e,:])[0]\n",
    "        for u in nodes_in_e:\n",
    "            for v in nodes_in_e:\n",
    "                A[u,v] += WE[e] * R[e,u] * R[e,v]\n",
    "    return A\n",
    "\n",
    "# create A, then find pagerank scores of random walk on A\n",
    "\n",
    "# get probability transition matrix\n",
    "A=compute_gh_weights(R, W, P)\n",
    "P = A/A.sum(axis=1)[:,None]\n",
    "P=P.T \n",
    "P = sparse.csr_matrix(P)\n",
    "\n",
    "# compute pagerank scores\n",
    "r=0.40\n",
    "rankings_gh = compute_pr(P, r, n, eps=1e-8).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KATZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "free_for_all_data = pd.read_csv('/Users/andy/Desktop/7332finalproj/hypergraph-halo-ranking/FreeForAll.csv')\n",
    "\n",
    "def compute_katz_centrality(A, alpha, beta, n, eps=1e-8, max_iter=1000):\n",
    "    \"\"\"\n",
    "    Compute Katz Centrality from an adjacency matrix.\n",
    "    Args:\n",
    "    - A: Adjacency matrix.\n",
    "    - alpha: Decay factor (must be < 1 / max_eigenvalue).\n",
    "    - beta: Bias term (typically 1).\n",
    "    - n: Number of nodes.\n",
    "    - eps: Convergence threshold.\n",
    "    - max_iter: Maximum number of iterations.\n",
    "    Returns:\n",
    "    - Katz Centrality scores for all nodes.\n",
    "    \"\"\"\n",
    "    x = np.ones(n) * beta\n",
    "    for _ in range(max_iter):\n",
    "        x_new = alpha * A.dot(x) + beta\n",
    "        if np.linalg.norm(x_new - x, ord=1) < eps:\n",
    "            break\n",
    "        x = x_new\n",
    "    return x\n",
    "\n",
    "def compute_katz_from_ffa(free_for_all_data):\n",
    "    \"\"\"\n",
    "    Compute Katz Centrality using the Free-for-All dataset.\n",
    "    Args:\n",
    "    - free_for_all_data: DataFrame containing Free-for-All matches.\n",
    "    Returns:\n",
    "    - Katz Centrality scores for all players.\n",
    "    \"\"\"\n",
    "    players = free_for_all_data.iloc[:, 4].unique() # id\n",
    "    player_indices = {player: idx for idx, player in enumerate(players)}\n",
    "    n = len(players)\n",
    "    A = np.zeros((n, n))  # adjacency matrix\n",
    "\n",
    "    grouped = free_for_all_data.groupby(free_for_all_data.iloc[:, 1])  # group by matchID\n",
    "    for _, match_data in grouped:\n",
    "        players_in_match = match_data.iloc[:, 4].values  # playerID\n",
    "        scores_in_match = match_data.iloc[:, 5].values  # score\n",
    "        for i in range(len(players_in_match)):\n",
    "            for j in range(len(players_in_match)):\n",
    "                if i != j:  # remove self-loops# need to check here\n",
    "                    A[player_indices[players_in_match[i]], player_indices[players_in_match[j]]] += scores_in_match[i]\n",
    "\n",
    "    lambda_max = eigs(A, k=1, which='LM', return_eigenvectors=False)[0].real\n",
    "    alpha = 0.1 / lambda_max\n",
    "    beta = 1.0\n",
    "    return compute_katz_centrality(A, alpha, beta, n)\n",
    "\n",
    "katz_rankings = compute_katz_from_ffa(free_for_all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EIGEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_eigenvector_centrality(A, n, eps=1e-8, max_iter=1000):\n",
    "    \"\"\"\n",
    "    Compute Eigenvector Centrality from an adjacency matrix.\n",
    "    Args:\n",
    "    - A: Adjacency matrix.\n",
    "    - n: Number of nodes.\n",
    "    - eps: Convergence threshold.\n",
    "    - max_iter: Maximum number of iterations.\n",
    "    Returns:\n",
    "    - Eigenvector Centrality scores for all nodes.\n",
    "    \"\"\"\n",
    "    x = np.ones(n) / n  # init scores\n",
    "    for _ in range(max_iter):\n",
    "        x_new = A.dot(x)\n",
    "        x_new /= np.linalg.norm(x_new, ord=1)\n",
    "        if np.linalg.norm(x_new - x, ord=1) < eps:\n",
    "            break\n",
    "        x = x_new\n",
    "    return x\n",
    "\n",
    "def compute_eigenvector_from_ffa(free_for_all_data):\n",
    "    \"\"\"\n",
    "    Compute Eigenvector Centrality using the Free-for-All dataset.\n",
    "    Args:\n",
    "    - free_for_all_data: DataFrame containing Free-for-All matches.\n",
    "    Returns:\n",
    "    - Eigenvector Centrality scores for all players.\n",
    "    \"\"\"\n",
    "    players = free_for_all_data.iloc[:, 4].unique()  \n",
    "    player_indices = {player: idx for idx, player in enumerate(players)}\n",
    "    n = len(players)\n",
    "    A = np.zeros((n, n)) \n",
    "\n",
    "    grouped = free_for_all_data.groupby(free_for_all_data.iloc[:, 1]) \n",
    "    for _, match_data in grouped:\n",
    "        players_in_match = match_data.iloc[:, 4].values\n",
    "        scores_in_match = match_data.iloc[:, 5].values\n",
    "        for i in range(len(players_in_match)):\n",
    "            for j in range(len(players_in_match)):\n",
    "                if i != j:\n",
    "                    A[player_indices[players_in_match[i]], player_indices[players_in_match[j]]] += scores_in_match[i]\n",
    "\n",
    "    return compute_eigenvector_centrality(A, n)\n",
    "\n",
    "eigenvector_rankings = compute_eigenvector_from_ffa(free_for_all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MC3 rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T23:12:39.785260Z",
     "start_time": "2019-05-11T23:10:47.456019Z"
    }
   },
   "outputs": [],
   "source": [
    "n = len(universe)\n",
    "\n",
    "Pd = np.zeros([n, n]) # d for dwork\n",
    "\n",
    "for i in universe:\n",
    "    i_counts = np.zeros(n) # number of ways i can go to any other vertex\n",
    "    i_deg = 0 #number of hyperedges where i can traverse to some other vertex\n",
    "\n",
    "    i_index = np.where(universe == i)\n",
    "    for pi, scores in pi_list:\n",
    "        if i in pi and len(pi) > 1:\n",
    "            pi_filtered = pi[pi.index(i)+1:] #everything ranked better than i\n",
    "\n",
    "            # if i can use this hyperedge\n",
    "            if len(pi_filtered) > 0:\n",
    "                # essentially, for each j in pi_filtered\n",
    "                # grab k=universe.index(j) and increment i_counts[k] by 1/len(pi)\n",
    "                i_counts[np.where(np.isin(universe, pi_filtered))] += 1/len(pi)\n",
    "\n",
    "            i_counts[i_index] += 1 - (len(pi_filtered) / len(pi))\n",
    "            i_deg += 1\n",
    "    if i_deg > 0:\n",
    "        i_counts /= i_deg\n",
    "    else:\n",
    "        i_counts[i_index] = 1\n",
    "    Pd[i_index,:] = i_counts\n",
    "\n",
    "Pd = np.transpose(Pd) # since we're using column vectors\n",
    "Pd = sparse.csr_matrix(Pd)\n",
    "\n",
    "# create MC3 rankings\n",
    "r=0.40\n",
    "\n",
    "rankings_mc3 = compute_pr(Pd, r, n, eps=1e-8).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create TrueSkill rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T23:15:48.240537Z",
     "start_time": "2019-05-11T23:15:48.220992Z"
    }
   },
   "outputs": [],
   "source": [
    "# simulate the change in TrueSkill ratings when a Free-For-All match is played\n",
    "def play_match(match, ts_ranking):\n",
    "    p, s = match\n",
    "    cur_ranks = []\n",
    "    for player in p:\n",
    "        if player in ts_ranking:\n",
    "            cur_ranks.append([ts_ranking[player]])\n",
    "        else:\n",
    "            cur_ranks.append([trueskill.Rating()])\n",
    "    # lower rank = better player for trueskill.rate function, so we turn scores into -1*scores\n",
    "    match_res = trueskill.rate(cur_ranks, ranks=[-1*i for i in s])\n",
    "    for i in range(len(p)):\n",
    "        player = p[i]\n",
    "        ts_ranking[player] = match_res[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T23:17:12.247092Z",
     "start_time": "2019-05-11T23:15:48.827252Z"
    }
   },
   "outputs": [],
   "source": [
    "trueskill_rankings={} # dict mapping player -> TrueSkill rating object\n",
    "\n",
    "# simulate all matches being played, in order\n",
    "for match in matches:\n",
    "    play_match(match, trueskill_rankings)\n",
    "\n",
    "rankings_ts = [trueskill_rankings[player].mu for player in players] # deterministic TrueSkill ratings list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hybrid model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine rankings into a hybrid score\n",
    "def compute_hybrid_ranking(rankings, weights):\n",
    "    \"\"\"\n",
    "    Compute hybrid rankings by combining weighted scores.\n",
    "    Args:\n",
    "    - rankings: Dictionary with keys as method names and values as ranking lists.\n",
    "    - weights: Dictionary with weights for each method.\n",
    "    Returns:\n",
    "    - hybrid_scores: Combined hybrid scores for each player.\n",
    "    \"\"\"\n",
    "    n = len(next(iter(rankings.values())))  # Get number of players\n",
    "    hybrid_scores = np.zeros(n)\n",
    "    for method, scores in rankings.items():\n",
    "        hybrid_scores += weights[method] * np.array(scores)\n",
    "    return hybrid_scores\n",
    "\n",
    "# Example weights for each method\n",
    "weights = {\n",
    "    \"Hypergraph\": 0.15,\n",
    "    \"G^H\": 0.25,\n",
    "    \"MC3\": 0.25,\n",
    "    \"TrueSkill\": 0.25\n",
    "}\n",
    "\n",
    "# Compute hybrid rankings\n",
    "rankings_combined = {\n",
    "    \"Hypergraph\": rankings_hg,\n",
    "    \"G^H\": rankings_gh,\n",
    "    \"MC3\": rankings_mc3,\n",
    "    \"TrueSkill\": rankings_ts\n",
    "}\n",
    "rankings_hybrid = compute_hybrid_ranking(rankings_combined, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elo Ranking Implementation\n",
    "def update_elo(player_elo, winner, loser, k=32):\n",
    "    \"\"\"\n",
    "    Update Elo ratings for a winner and a loser.\n",
    "    Args:\n",
    "    - player_elo: Dictionary of player Elo ratings.\n",
    "    - winner: ID of the winner.\n",
    "    - loser: ID of the loser.\n",
    "    - k: K-factor for adjusting Elo ratings.\n",
    "    \"\"\"\n",
    "    r1 = 10 ** (player_elo[winner] / 400)\n",
    "    r2 = 10 ** (player_elo[loser] / 400)\n",
    "    e1 = r1 / (r1 + r2)\n",
    "    e2 = r2 / (r1 + r2)\n",
    "    player_elo[winner] += k * (1 - e1)\n",
    "    player_elo[loser] += k * (0 - e2)\n",
    "\n",
    "def compute_elo_rankings(free_for_all_data):\n",
    "    \"\"\"\n",
    "    Compute Elo rankings using the Free-for-All dataset.\n",
    "    Args:\n",
    "    - free_for_all_data: DataFrame containing Free-for-All matches.\n",
    "    Returns:\n",
    "    - Dictionary of Elo scores for all players.\n",
    "    \"\"\"\n",
    "    player_elo = {}  # Initialize Elo ratings\n",
    "    grouped = free_for_all_data.groupby(free_for_all_data.iloc[:, 1])  # Group by MatchID\n",
    "    for _, match_data in grouped:\n",
    "        players = match_data.iloc[:, 4].values  # PlayerID\n",
    "        scores = match_data.iloc[:, 5].values  # Score\n",
    "        for i in range(len(players)):\n",
    "            for j in range(i + 1, len(players)):\n",
    "                if scores[i] != scores[j]:  # Avoid ties\n",
    "                    winner = players[i] if scores[i] > scores[j] else players[j]\n",
    "                    loser = players[j] if scores[i] > scores[j] else players[i]\n",
    "                    if winner not in player_elo:\n",
    "                        player_elo[winner] = 1000\n",
    "                    if loser not in player_elo:\n",
    "                        player_elo[loser] = 1000\n",
    "                    update_elo(player_elo, winner, loser)\n",
    "    return player_elo\n",
    "\n",
    "elo_rankings_dict = compute_elo_rankings(free_for_all_data)\n",
    "elo_rankings = [elo_rankings_dict[player] if player in elo_rankings_dict else 0 for player in players]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# katz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import eigs\n",
    "\n",
    "def compute_katz_centrality(A, alpha, beta, n, eps=1e-8, max_iter=1000):\n",
    "    \"\"\"\n",
    "    Compute Katz Centrality.\n",
    "    A: Adjacency matrix\n",
    "    alpha: Decay factor (must be less than 1 / max_eigenvalue)\n",
    "    beta: Bias term (usually set to 1)\n",
    "    \"\"\"\n",
    "    x = np.ones(n) * beta\n",
    "    for _ in range(max_iter):\n",
    "        x_new = alpha * A.dot(x) + beta\n",
    "        if np.linalg.norm(x_new - x, ord=1) < eps:\n",
    "            break\n",
    "        x = x_new\n",
    "    return x\n",
    "\n",
    "# Use adjacency matrix A from G^H\n",
    "lambda_max = eigs(A, k=1, which='LM', return_eigenvectors=False)[0].real\n",
    "alpha = 0.1 / lambda_max\n",
    "beta = 1.0\n",
    "rankings_katz = compute_katz_centrality(A, alpha, beta, len(players))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eigenvector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_eigenvector_centrality(A, n):\n",
    "    \"\"\"\n",
    "    Compute Eigenvector Centrality.\n",
    "    A: Adjacency matrix\n",
    "    \"\"\"\n",
    "    eigenvalues, eigenvectors = eigs(A, k=1, which='LR')\n",
    "    centrality = eigenvectors[:, 0].real\n",
    "    centrality = np.abs(centrality)  # Avoid negative values\n",
    "    centrality /= centrality.sum()  # Normalize\n",
    "    return centrality\n",
    "\n",
    "rankings_eigenvector = compute_eigenvector_centrality(A, len(players))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glicko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glicko constants\n",
    "q = math.log(10) / 400  # Scaling factor\n",
    "g = lambda RD: 1 / math.sqrt(1 + (3 * (q ** 2) * (RD ** 2)) / (math.pi ** 2))  # RD scaling factor\n",
    "E = lambda R1, R2, RD2: 1 / (1 + 10 ** (-g(RD2) * (R1 - R2) / 400))  # Expected score\n",
    "\n",
    "def update_glicko(player_ratings, matches, tau=0.5):\n",
    "    \"\"\"\n",
    "    Update Glicko ratings for players based on match outcomes.\n",
    "    Args:\n",
    "    - player_ratings: Dictionary of player ratings as (R, RD, volatility).\n",
    "    - matches: List of matches as (player1, player2, outcome), where:\n",
    "               - player1, player2 are IDs\n",
    "               - outcome: 1 if player1 wins, 0 if player2 wins\n",
    "    - tau: System constant for volatility adjustment (default 0.5).\n",
    "    Returns:\n",
    "    - Updated player ratings.\n",
    "    \"\"\"\n",
    "    for player1, player2, outcome in matches:\n",
    "        if player1 not in player_ratings:\n",
    "            player_ratings[player1] = (1500, 350, 0.06)  # Default values\n",
    "        if player2 not in player_ratings:\n",
    "            player_ratings[player2] = (1500, 350, 0.06)\n",
    "\n",
    "        R1, RD1, sigma1 = player_ratings[player1]\n",
    "        R2, RD2, sigma2 = player_ratings[player2]\n",
    "\n",
    "        # Convert ratings to Glicko scale\n",
    "        mu1, mu2 = R1 / 173.7178, R2 / 173.7178\n",
    "        phi1, phi2 = RD1 / 173.7178, RD2 / 173.7178\n",
    "\n",
    "        # Step 1: Calculate g(RD2) and E(mu1, mu2)\n",
    "        g_phi2 = g(phi2)\n",
    "        E_mu1_mu2 = 1 / (1 + math.exp(-g_phi2 * (mu1 - mu2)))\n",
    "\n",
    "        # Step 2: Update rating deviation\n",
    "        v = (g_phi2 ** 2) * E_mu1_mu2 * (1 - E_mu1_mu2)\n",
    "        delta = g_phi2 * (outcome - E_mu1_mu2) / v\n",
    "        phi_prime = 1 / math.sqrt(1 / (phi1 ** 2) + 1 / v)\n",
    "\n",
    "        # Step 3: Update rating\n",
    "        mu_prime = mu1 + phi_prime ** 2 * g_phi2 * (outcome - E_mu1_mu2)\n",
    "\n",
    "        # Step 4: Update volatility (simplified for this example)\n",
    "        sigma_prime = sigma1  # Volatility update omitted for simplicity\n",
    "\n",
    "        # Convert back to original scale\n",
    "        R_prime = mu_prime * 173.7178\n",
    "        RD_prime = phi_prime * 173.7178\n",
    "\n",
    "        player_ratings[player1] = (R_prime, RD_prime, sigma_prime)\n",
    "\n",
    "    return player_ratings\n",
    "\n",
    "def compute_glicko_rankings(free_for_all_data):\n",
    "    \"\"\"\n",
    "    Compute Glicko rankings using the Free-for-All dataset.\n",
    "    Args:\n",
    "    - free_for_all_data: DataFrame containing Free-for-All matches.\n",
    "    Returns:\n",
    "    - Dictionary of Glicko scores for all players.\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    grouped = free_for_all_data.groupby(free_for_all_data.iloc[:, 1])  # Group by MatchID\n",
    "    for _, match_data in grouped:\n",
    "        players = match_data.iloc[:, 4].values  # PlayerID\n",
    "        scores = match_data.iloc[:, 5].values  # Score\n",
    "        for i in range(len(players)):\n",
    "            for j in range(i + 1, len(players)):\n",
    "                if scores[i] != scores[j]:  # Avoid ties\n",
    "                    outcome = 1 if scores[i] > scores[j] else 0\n",
    "                    matches.append((players[i], players[j], outcome))\n",
    "    # Initialize player ratings\n",
    "    player_ratings = {}\n",
    "    player_ratings = update_glicko(player_ratings, matches)\n",
    "    return {player: rating[0] for player, rating in player_ratings.items()}  # Return only ratings\n",
    "\n",
    "# Compute Glicko Rankings\n",
    "glicko_rankings_dict = compute_glicko_rankings(free_for_all_data)\n",
    "glicko_rankings = [glicko_rankings_dict[player] if player in glicko_rankings_dict else 0 for player in players]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate 4 rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T23:17:20.012311Z",
     "start_time": "2019-05-11T23:17:19.982816Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluating a 1v1 game with a deterministic ranking of players\n",
    "# INPUTS:\n",
    "# game_players: list of players in the match\n",
    "# game_scores: list of scores in the match (corresponding to game_players)\n",
    "# all_players: list of all players in all matches\n",
    "# ranks: one of the 4 rankings computed above\n",
    "\n",
    "# OUTPUT: \n",
    "# can_eval: False if game ends in tie, True otherwise (we ignore tie games)\n",
    "# res: 1 if ranks correctly predicts the match, 0 if not\n",
    "def eval_game_h2h(game_players, game_scores, all_players, ranks):\n",
    "    players_ranked_prev = [player for player in game_players if player in all_players]\n",
    "    if len(players_ranked_prev) == 2:\n",
    "        # get scores for players previously ranked\n",
    "        scores_prev = [game_scores[game_players.index(player)] for player in players_ranked_prev]\n",
    "        ranks_prev = [ranks[all_players.index(player)] for player in players_ranked_prev]\n",
    "\n",
    "        # make sure there isn't a tie\n",
    "        if scores_prev[0] != scores_prev[1]:\n",
    "            can_eval = True\n",
    "            \n",
    "            # check if ranked correctly\n",
    "            if sum(np.argsort(scores_prev) == np.argsort(ranks_prev)) == 2:\n",
    "                res = True\n",
    "            else:\n",
    "                res = False\n",
    "        else:\n",
    "            can_eval = False\n",
    "            res = False\n",
    "    else:\n",
    "        can_eval = False\n",
    "        res = False\n",
    "    return (can_eval, int(res))\n",
    "\n",
    "# Evaluating a 1v1 game with TS probabilistic procedure. Same inputs/outputs as above.\n",
    "def eval_game_h2h_trueskill(game_players, game_scores, all_players, ts_ranking):\n",
    "    players_ranked_prev = [player for player in game_players if player in all_players]\n",
    "    if len(players_ranked_prev) == 2:\n",
    "        # get scores for players previously ranked\n",
    "        scores_prev = [game_scores[game_players.index(player)] for player in players_ranked_prev]\n",
    "        ts_ranks_prev = [ts_ranking[player] for player in players_ranked_prev]\n",
    "\n",
    "        # make sure there isn't a tie\n",
    "        if scores_prev[0] != scores_prev[1]:\n",
    "            can_eval = True\n",
    "            \n",
    "            # compare rating distributions between two players \n",
    "            # (for simplicity, do not consider draw probability)\n",
    "            mu0, sigma0 = ts_ranks_prev[0]\n",
    "            mu1, sigma1 = ts_ranks_prev[1]\n",
    "            p = 1 - norm.cdf(-1.0 * (mu0 - mu1) / (sigma0**2 + sigma1**2))\n",
    "            if (p > 0.5 and scores_prev[0] > scores_prev[1]) or (p < 0.5 and scores_prev[0] < scores_prev[1]):\n",
    "                res = True\n",
    "            else:\n",
    "                res = False\n",
    "\n",
    "        else:\n",
    "            can_eval = False\n",
    "            res = False\n",
    "    else:\n",
    "        can_eval = False\n",
    "        res = False\n",
    "    return (can_eval, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.optimize import minimize\n",
    "# import numpy as np\n",
    "\n",
    "# # Define a function to compute hybrid rankings with given weights\n",
    "# def compute_hybrid_ranking(weights, rankings):\n",
    "#     \"\"\"\n",
    "#     Compute hybrid rankings by combining weighted scores.\n",
    "#     Args:\n",
    "#     - weights: Array of weights for each ranking method.\n",
    "#     - rankings: Dictionary with keys as method names and values as ranking lists.\n",
    "#     Returns:\n",
    "#     - hybrid_scores: Combined hybrid scores for each player.\n",
    "#     \"\"\"\n",
    "#     n = len(next(iter(rankings.values())))  # Number of players\n",
    "#     hybrid_scores = np.zeros(n)\n",
    "#     for i, (method, scores) in enumerate(rankings.items()):\n",
    "#         hybrid_scores += weights[i] * np.array(scores)\n",
    "#     return hybrid_scores\n",
    "\n",
    "# # Define an objective function to minimize (negative accuracy for maximization)\n",
    "# def objective_function(weights, rankings, players, eval_func, games):\n",
    "#     \"\"\"\n",
    "#     Objective function to minimize negative prediction accuracy.\n",
    "#     Args:\n",
    "#     - weights: Array of weights for each ranking method.\n",
    "#     - rankings: Dictionary of rankings from different methods.\n",
    "#     - players: List of all players.\n",
    "#     - eval_func: Evaluation function to compute prediction accuracy.\n",
    "#     - games: List of games with player and score data.\n",
    "#     Returns:\n",
    "#     - Negative accuracy (to maximize accuracy by minimizing this value).\n",
    "#     \"\"\"\n",
    "#     weights = np.array(weights) / np.sum(weights)  # Normalize weights\n",
    "#     hybrid_scores = compute_hybrid_ranking(weights, rankings)\n",
    "#     correct_predictions = 0\n",
    "#     total_games = 0\n",
    "\n",
    "#     for game_players, game_scores in games:\n",
    "#         can_eval, result = eval_func(game_players, game_scores, players, hybrid_scores)\n",
    "#         if can_eval:\n",
    "#             correct_predictions += result\n",
    "#             total_games += 1\n",
    "\n",
    "#     accuracy = correct_predictions / total_games if total_games > 0 else 0\n",
    "#     return -accuracy  # Negate accuracy for minimization\n",
    "\n",
    "# # Example usage\n",
    "# rankings_combined = {\n",
    "#     \"Hypergraph\": rankings_hg,\n",
    "#     \"G^H\": rankings_gh,\n",
    "#     \"MC3\": rankings_mc3,\n",
    "#     \"TrueSkill\": rankings_ts\n",
    "# }\n",
    "\n",
    "\n",
    "# # Initialize variables to hold all players and scores from the dataset\n",
    "# all_game_players = []\n",
    "# all_game_scores = []\n",
    "\n",
    "# # Populate all_game_players and all_game_scores during game parsing\n",
    "# with open('/Users/andy/Desktop/7332finalproj/hypergraph-halo-ranking/HeadToHead.csv') as csv_file:\n",
    "#     csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "#     cur_game = -1\n",
    "#     cur_players = []\n",
    "#     cur_scores = []\n",
    "\n",
    "#     for row in csv_reader:\n",
    "#         game = int(row[1])\n",
    "#         player = row[4]\n",
    "#         score = int(row[6])\n",
    "\n",
    "#         if game == cur_game:\n",
    "#             cur_players.append(player)\n",
    "#             cur_scores.append(score)\n",
    "#         else:\n",
    "#             if cur_game > 0 and np.sum(np.abs(cur_scores)) > 0:\n",
    "#                 # Add completed game's players and scores\n",
    "#                 all_game_players.append(cur_players)\n",
    "#                 all_game_scores.append(cur_scores)\n",
    "\n",
    "#             # Reset for the new game\n",
    "#             cur_game = game\n",
    "#             cur_players = [player]\n",
    "#             cur_scores = [score]\n",
    "\n",
    "#     # Add the last game\n",
    "#     if np.sum(np.abs(cur_scores)) > 0:\n",
    "#         all_game_players.append(cur_players)\n",
    "#         all_game_scores.append(cur_scores)\n",
    "\n",
    "# # Create the games list\n",
    "# games = [(cur_players, cur_scores) for cur_players, cur_scores in zip(all_game_players, all_game_scores)]\n",
    "# # Define players if missing\n",
    "# players = list(set(player for game in all_game_players for player in game))\n",
    "\n",
    "# # Initial weights\n",
    "# initial_weights = np.ones(len(rankings_combined))\n",
    "\n",
    "# # Minimize the negative accuracy to find the best weights\n",
    "# result = minimize(\n",
    "#     objective_function,\n",
    "#     initial_weights,\n",
    "#     args=(rankings_combined, players, eval_game_h2h, games),\n",
    "#     method='SLSQP',\n",
    "#     bounds=[(0, 1)] * len(rankings_combined),  # Ensure weights are between 0 and 1\n",
    "#     constraints={'type': 'eq', 'fun': lambda w: np.sum(w) - 1}  # Weights sum to 1\n",
    "# )\n",
    "\n",
    "# # Get the optimal weights\n",
    "# optimal_weights = result.x\n",
    "# print(f\"Optimal weights: {optimal_weights}\")\n",
    "\n",
    "# # Compute final hybrid rankings with optimal weights\n",
    "# rankings_hybrid = compute_hybrid_ranking(optimal_weights, rankings_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go through each game and use each of the different rankings to predict the winner. Compare to actual winner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T23:17:44.757611Z",
     "start_time": "2019-05-11T23:17:24.344544Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypergraph accuracy: 0.7113685450618495\n",
      "Glicko accuracy: 0.49518947575103084\n",
      "Clique Graph accuracy: 0.6112311015118791\n",
      "Dwork MC3 accuracy: 0.5293540153151384\n",
      "TrueSkill accuracy: 0.7345376006283134\n",
      "Hybrid accuracy: 0.7345376006283134\n",
      "TrueSkill accuracy, probabilistic decision procedure: 0.7345376006283134\n",
      "Elo accuracy: 0.49518947575103084\n",
      "Katz Centrality accuracy: 0.49734930296485375\n",
      "Eigenvector Centrality accuracy: 0.49931278225014725\n"
     ]
    }
   ],
   "source": [
    "cur_game = -1\n",
    "cur_players = []\n",
    "cur_scores = []\n",
    "\n",
    "results_hg=[]\n",
    "results_gh=[]\n",
    "results_mc3=[]\n",
    "results_ts=[]\n",
    "results_ts_prob=[]\n",
    "results_hybrid = []\n",
    "results_elo = []\n",
    "results_katz = []\n",
    "results_eigenvector = []\n",
    "results_glicko = []\n",
    "\n",
    "with open('/Users/andy/Desktop/7332finalproj/hypergraph-halo-ranking/HeadToHead.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    for row in csv_reader:\n",
    "        game = int(row[1])\n",
    "        player = row[4]\n",
    "        score = int(row[6])\n",
    "\n",
    "        # next, decide if this row is from the same match\n",
    "        # as the last row, or a different match\n",
    "        if game == cur_game:\n",
    "            cur_players.append(player)\n",
    "            cur_scores.append(score)\n",
    "        else:\n",
    "            if cur_game > 0 and np.sum(np.abs(cur_scores)) > 0:\n",
    "                # evaluate game\n",
    "               # Evaluate game\n",
    "                can_eval, elo_match_res = eval_game_h2h(cur_players, cur_scores, players, elo_rankings)\n",
    "                can_eval, katz_match_res = eval_game_h2h(cur_players, cur_scores, players, katz_rankings)\n",
    "                can_eval, eigenvector_match_res = eval_game_h2h(cur_players, cur_scores, players, eigenvector_rankings)\n",
    "\n",
    "                can_eval, hybrid_match_res = eval_game_h2h(cur_players, cur_scores, players, rankings_hybrid)\n",
    "                can_eval, hg_match_res = eval_game_h2h(cur_players, cur_scores, players, rankings_hg)\n",
    "                can_eval, gh_match_res = eval_game_h2h(cur_players, cur_scores, players, rankings_gh)\n",
    "                can_eval, mc3_match_res = eval_game_h2h(cur_players, cur_scores, players, rankings_mc3)\n",
    "                can_eval, ts_match_res = eval_game_h2h(cur_players, cur_scores, players, rankings_ts)\n",
    "                can_eval, ts_prob_match_res = eval_game_h2h_trueskill(cur_players, cur_scores, players, trueskill_rankings)\n",
    "                can_eval, glicko_match_res = eval_game_h2h(cur_players, cur_scores, players, glicko_rankings)\n",
    "                if can_eval:\n",
    "                    results_elo.append(elo_match_res)\n",
    "                    results_katz.append(katz_match_res)\n",
    "                    results_eigenvector.append(eigenvector_match_res)\n",
    "                    results_hg.append(hg_match_res)\n",
    "                    results_gh.append(gh_match_res)\n",
    "                    results_mc3.append(mc3_match_res)\n",
    "                    results_ts.append(ts_match_res)\n",
    "                    results_hybrid.append(hybrid_match_res)\n",
    "                    results_ts_prob.append(ts_prob_match_res)\n",
    "                    results_glicko.append(glicko_match_res)\n",
    "\n",
    "            # reset cur_game, cur_players, cur_scores\n",
    "            cur_game = game\n",
    "            cur_players = [player]\n",
    "            cur_scores = [score]\n",
    "\n",
    "num_games = len(results_hg)\n",
    "\n",
    "# Print accuracy for all ranking methods\n",
    "print('Hypergraph accuracy: {}'.format(sum(results_hg) * 1.0 / num_games))\n",
    "print('Glicko accuracy: {}'.format(sum(results_glicko) * 1.0 / num_games))\n",
    "print('Clique Graph accuracy: {}'.format(sum(results_gh) * 1.0 / num_games))\n",
    "print('Dwork MC3 accuracy: {}'.format(sum(results_mc3) * 1.0 / num_games))\n",
    "print('TrueSkill accuracy: {}'.format(sum(results_ts) * 1.0 / num_games))\n",
    "print('Hybrid accuracy: {}'.format(sum(results_hybrid) * 1.0 / num_games))\n",
    "print('TrueSkill accuracy, probabilistic decision procedure: {}'.format(sum(results_ts_prob) * 1.0 / num_games))\n",
    "print('Elo accuracy: {}'.format(sum(results_elo) * 1.0 / num_games))\n",
    "print('Katz Centrality accuracy: {}'.format(sum(results_katz) * 1.0 / num_games))\n",
    "print('Eigenvector Centrality accuracy: {}'.format(sum(results_eigenvector) * 1.0 / num_games))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differences between hypergraph and TS rankings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the percentage of matches which HG/TS differ on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T23:18:44.212622Z",
     "start_time": "2019-05-11T23:18:44.201501Z"
    }
   },
   "outputs": [],
   "source": [
    "hg_only=0\n",
    "ts_only=0\n",
    "both=0\n",
    "for i in range(num_games):\n",
    "    if results_hg[i] > 0 and results_ts[i] > 0:\n",
    "        both += 1\n",
    "    elif results_hg[i] > 0:\n",
    "        hg_only += 1\n",
    "    elif results_ts[i] > 0:\n",
    "        ts_only += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T23:18:44.832459Z",
     "start_time": "2019-05-11T23:18:44.825829Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of Matches predicted correctly by both TrueSkill and hypergraph: 0.6220302375809935\n",
      "% of Matches predicted correctly by only TrueSkill: 0.11250736304731986\n",
      "% of Matches predicted correctly by only hypergraph: 0.08933830748085608\n"
     ]
    }
   ],
   "source": [
    "print('% of Matches predicted correctly by both TrueSkill and hypergraph: {}'.format(both / num_games))\n",
    "print('% of Matches predicted correctly by only TrueSkill: {}'.format(ts_only / num_games))\n",
    "print('% of Matches predicted correctly by only hypergraph: {}'.format(hg_only / num_games))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1rc1"
  },
  "notify_time": "0",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
