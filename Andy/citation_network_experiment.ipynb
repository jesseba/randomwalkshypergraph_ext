{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "from scipy.stats import weightedtau, kendalltau\n",
    "from scipy.stats import norm\n",
    "from scipy.linalg import null_space\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pprint import pprint\n",
    "from copy import deepcopy\n",
    "\n",
    "import trueskill\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# either one should work:\n",
    "\n",
    "# https://lfs.aminer.cn/lab-datasets/citation/dblp.v10.zip\n",
    "# https://lfs.aminer.cn/lab-datasets/citation/dblp.v10.ziphttps://lfs.aminer.cn/lab-datasets/citation/dblp.v10.zip\n",
    "\n",
    "zip_path = './data/dblp.v10.zip'\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "    json_files = [f for f in z.namelist() if f.endswith('.json')]\n",
    "    \n",
    "    if len(json_files) != 4:\n",
    "        raise ValueError(\"There are not exactly four JSON files in the zip.\")\n",
    "    \n",
    "    dataframes = []\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        # Read the json file content into a pandas DataFrame\n",
    "        with z.open(json_file) as f:\n",
    "            data = pd.read_json(io.BytesIO(f.read()), lines=True)\n",
    "        \n",
    "        \n",
    "        dataframes.append(data)\n",
    "\n",
    "df, df2, df3, df4 = dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep thes conferences only\n",
    "conferences = [\n",
    "    \"neural information processing systems\",\n",
    "    \"international conference on machine learning\",\n",
    "    \"knowledge discovery and data mining\",\n",
    "   \"international joint conference on artificial intelligence\",\n",
    "   \"uncertainty in artificial intelligence\",\n",
    "    #\"conference on uncertainty in artificial intelligence\",\n",
    "    \"international conference on learning representations\",\n",
    "    \"computational learning theory\"\n",
    "]\n",
    "\n",
    "\n",
    "def filter_venues(df):\n",
    "    df['venue'] = df['venue'].str.lower()\n",
    "    return df[df['venue'].isin(conferences)]\n",
    "\n",
    "\n",
    "filtered_dfs = [filter_venues(df) for df in [df, df2, df3, df4]]\n",
    "combined_filtered_df = pd.concat(filtered_dfs, ignore_index=True)\n",
    "\n",
    "combined_filtered_df= combined_filtered_df[combined_filtered_df['authors'].notna()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_alphabetically_sorted(authors_list): # check if it is alphabatically sorted\n",
    "    if not authors_list:  \n",
    "        return False\n",
    "    sorted_authors = sorted(authors_list)\n",
    "    return authors_list == sorted_authors\n",
    "\n",
    "\n",
    "def assign_weights(authors): # if alphbatically placed then 1 for all, otherwise follow the literature\n",
    "    if is_alphabetically_sorted(authors):\n",
    "       \n",
    "        return [1] * len(authors)\n",
    "    else:\n",
    "        \n",
    "        if len(authors) == 1:\n",
    "            \n",
    "            return [2]\n",
    "        else:\n",
    "            \n",
    "            weights = [1] * len(authors)\n",
    "            weights[0] = 2  # First author weight\n",
    "            weights[-1] = 2  # Last author weight\n",
    "            return weights\n",
    "\n",
    "        \n",
    "def assign_uni_weights(authors): \n",
    "    return [1] * len(authors)\n",
    "\n",
    "\n",
    "\n",
    "citation_list = []\n",
    "citation_list_ind = []\n",
    "for authors in combined_filtered_df['authors']:\n",
    "    if isinstance(authors, list): \n",
    "        weights = assign_weights(authors)\n",
    "        uni_weights = assign_uni_weights(authors)\n",
    "        # for vertex dependent \n",
    "        citation_list.append((authors, weights))\n",
    "        # for vertex ind\n",
    "        citation_list_ind.append((authors, uni_weights))\n",
    "    else:\n",
    "        citation_list.append((authors, []))\n",
    "        citation_list_ind.append((authors, []))  \n",
    "\n",
    "\n",
    "# get the universse\n",
    "universe = set()\n",
    "\n",
    "for authors, _ in citation_list:\n",
    "    universe.update(authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_list = citation_list\n",
    "#pi_list_ind = citation_list_ind\n",
    "universe = np.array(list(universe))\n",
    "# first create these matrices\n",
    "# R = |E| x |V|, R(e, v) = lambda_e(v)\n",
    "# W = |V| x |E|, W(v, e) = w(e) 1(v in e)\n",
    "\n",
    "m = len(pi_list) # number of hyperedges\n",
    "n = len(universe) # number of items to be ranked \n",
    "R = np.zeros([m, n])\n",
    "W = np.zeros([n, m])\n",
    "\n",
    "for i in range(len(pi_list)):\n",
    "    pi, scores = pi_list[i]\n",
    "    if len(pi) > 1:   \n",
    "        for j in range(len(pi)):\n",
    "            v = pi[j]\n",
    "            v = np.where(universe == v)[0][0] #equivalent to universe.index(v) but for np arrays\n",
    "            #R[i, v] = np.exp(scores[j])\n",
    "            R[i, v] = scores[j]\n",
    "            W[v,i] = combined_filtered_df.iloc[:,2][i]+1# citation + 1\n",
    "\n",
    "        R[i, :] = R[i,:] / sum(R[i,:])\n",
    "\n",
    "        \n",
    "     \n",
    "\n",
    "W = np.nan_to_num(W, nan=0.0)\n",
    "\n",
    "# 计算每一行的和\n",
    "sum_W = W.sum(axis=1)\n",
    "# sanity chec, for those Nan and o, we replace them with 1\n",
    "zero_sum_rows = np.where(sum_W == 0)[0]\n",
    "nan_sum_rows = np.where(np.isnan(sum_W))[0]\n",
    "\n",
    "sum_W_corrected = sum_W.copy()\n",
    "sum_W_corrected[sum_W_corrected == 0] = 1\n",
    "\n",
    "# 归一化 W\n",
    "Wnorm = W / sum_W_corrected[:, None]  \n",
    "###\n",
    "\n",
    "\n",
    "\n",
    "# # first, normalize W\n",
    "# #Wnorm=W/W.sum(axis=1)[:,None]\n",
    "Ws = sparse.csr_matrix(Wnorm)\n",
    "Rs = sparse.csr_matrix(R)\n",
    "\n",
    "# create prob trans matrices\n",
    "P = np.transpose(Ws.dot(Rs))\n",
    "\n",
    "# create rankings\n",
    "r=0.40\n",
    "\n",
    "\n",
    "# COMPUTE PAGERANK\n",
    "##################################################\n",
    "\n",
    "# given probability transition matrix P\n",
    "# where P_{v,w} = Prob(w -> v)\n",
    "# find pagerank scores with restart probability r\n",
    "def compute_pr(P, r, n, eps=1e-8):\n",
    "    x = np.ones(n) / n*1.0\n",
    "    flag = True\n",
    "    t=0\n",
    "    while flag:\n",
    "        x_new = (1-r)*P*x\n",
    "        x_new = x_new + np.ones(n) * r / n\n",
    "        diff = np.linalg.norm(x_new - x)\n",
    "        if np.linalg.norm(x_new - x,ord=1) < eps and t > 100:\n",
    "            flag = False\n",
    "        t=t+1\n",
    "        x = x_new\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rankings_hg = compute_pr(P, r, n, eps=1e-8).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rankings_hg /= rankings_hg.sum()\n",
    "assert len(rankings_hg) == len(universe), \"rankings_hg and universe have diff length！\"\n",
    "\n",
    "ranking_df =  pd.DataFrame({\n",
    "    'Name': universe,\n",
    "    'PageRank_Score': rankings_hg\n",
    "})\n",
    "\n",
    "ranking_df_sorted = ranking_df.sort_values(by='PageRank_Score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# names in the literature\n",
    "bignames = ['Richard Socher', 'Zhongzhi Shi', 'Daniel Rueckert', \n",
    "                   'Lars Schmidt-Thieme','Tat-Seng Chua','Ian J. Goodfellow']\n",
    "\n",
    "bignames_only_hd = ranking_df_sorted[ranking_df_sorted['Name'].isin(bignames)]\n",
    "\n",
    "bignames_only_hd[\"H_D Rank\"] = bignames_only_hd.index\n",
    "\n",
    "\n",
    "bignames_only_hd.set_index(\"Name\", inplace=True)\n",
    "\n",
    "bignames_only_hd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just leave it as it is for now:\n",
    "pi_list = citation_list_ind\n",
    "#pi_list_ind = citation_list_ind\n",
    "universe = np.array(list(universe))\n",
    "# first create these matrices\n",
    "# R = |E| x |V|, R(e, v) = lambda_e(v)\n",
    "# W = |V| x |E|, W(v, e) = w(e) 1(v in e)\n",
    "\n",
    "m = len(pi_list) # number of hyperedges\n",
    "n = len(universe) # number of items to be ranked \n",
    "R = np.zeros([m, n])\n",
    "W = np.zeros([n, m])\n",
    "\n",
    "for i in range(len(pi_list)):\n",
    "    pi, scores = pi_list[i]\n",
    "    if len(pi) > 1:   \n",
    "        for j in range(len(pi)):\n",
    "            v = pi[j]\n",
    "            v = np.where(universe == v)[0][0] #equivalent to universe.index(v) but for np arrays\n",
    "            #R[i, v] = np.exp(scores[j])\n",
    "            R[i, v] = scores[j]\n",
    "            W[v,i] = combined_filtered_df.iloc[:,2][i]+1# citation + 1\n",
    "\n",
    "        R[i, :] = R[i,:] / sum(R[i,:])\n",
    "\n",
    "        \n",
    "     \n",
    "\n",
    "W = np.nan_to_num(W, nan=0.0)\n",
    "\n",
    "# 计算每一行的和\n",
    "sum_W = W.sum(axis=1)\n",
    "# sanity chec, for those Nan and o, we replace them with 1\n",
    "zero_sum_rows = np.where(sum_W == 0)[0]\n",
    "nan_sum_rows = np.where(np.isnan(sum_W))[0]\n",
    "\n",
    "sum_W_corrected = sum_W.copy()\n",
    "sum_W_corrected[sum_W_corrected == 0] = 1\n",
    "\n",
    "# 归一化 W\n",
    "Wnorm = W / sum_W_corrected[:, None]  \n",
    "###\n",
    "\n",
    "\n",
    "\n",
    "# # first, normalize W\n",
    "# #Wnorm=W/W.sum(axis=1)[:,None]\n",
    "Ws = sparse.csr_matrix(Wnorm)\n",
    "Rs = sparse.csr_matrix(R)\n",
    "\n",
    "# create prob trans matrices\n",
    "P = np.transpose(Ws.dot(Rs))\n",
    "\n",
    "# create rankings\n",
    "r=0.40\n",
    "\n",
    "\n",
    "# COMPUTE PAGERANK\n",
    "##################################################\n",
    "\n",
    "# given probability transition matrix P\n",
    "# where P_{v,w} = Prob(w -> v)\n",
    "# find pagerank scores with restart probability r\n",
    "def compute_pr(P, r, n, eps=1e-8):\n",
    "    x = np.ones(n) / n*1.0\n",
    "    flag = True\n",
    "    t=0\n",
    "    while flag:\n",
    "        x_new = (1-r)*P*x\n",
    "        x_new = x_new + np.ones(n) * r / n\n",
    "        diff = np.linalg.norm(x_new - x)\n",
    "        if np.linalg.norm(x_new - x,ord=1) < eps and t > 100:\n",
    "            flag = False\n",
    "        t=t+1\n",
    "        x = x_new\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rankings_hg_ind = compute_pr(P, r, n, eps=1e-8).flatten()\n",
    "\n",
    "\n",
    "#rankings_hg /= rankings_hg.sum()\n",
    "assert len(rankings_hg_ind) == len(universe), \"rankings_hg and universe have diff length！\"\n",
    "\n",
    "# 创建一个包含作者和对应分数的 DataFrame\n",
    "ranking_df =  pd.DataFrame({\n",
    "    'Name': universe,\n",
    "    'PageRank_Score': rankings_hg_ind\n",
    "})\n",
    "\n",
    "# 按照 PageRank 分数降序排序\n",
    "ranking_df_sorted = ranking_df.sort_values(by='PageRank_Score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "bignames = ['Richard Socher', 'Zhongzhi Shi', 'Daniel Rueckert', \n",
    "                   'Lars Schmidt-Thieme','Tat-Seng Chua','Ian J. Goodfellow']\n",
    "\n",
    "bignames_only_ht = ranking_df_sorted[ranking_df_sorted['Name'].isin(bignames)]\n",
    "\n",
    "# Replace `PageRank_Score` with the index\n",
    "bignames_only_ht[\"H_T Rank\"] = bignames_only_ht.index\n",
    "\n",
    "# Drop the old `PageRank_Score` column\n",
    "# df.drop(columns=[\"PageRank_Score\"], inplace=True)\n",
    "\n",
    "# # Set the `Name` column as the index\n",
    "bignames_only_ht.set_index(\"Name\", inplace=True)\n",
    "\n",
    "# # Display the updated DataFrame\n",
    "# print(df)\n",
    "\n",
    "bignames_only_ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau, p_value = kendalltau(rankings_hg_ind, rankings_hg)\n",
    "\n",
    "print(\"Kendall's Tau:\", tau)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
